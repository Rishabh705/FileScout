{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy opencv-python Pillow matplotlib python-dotenv face-recognition ipykernel transformers scikit-learn torch  torchvision  torchaudio sentence-transformers textblob python-docx pymupdf pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fde2995-3fe3-4859-b7bb-51bed225ed84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Universal Search Engine for PC - Functional Programming Version.\n",
    "\n",
    "This module provides functionality to search for images, faces, text in images, and files\n",
    "on your local computer. It uses various techniques like object detection, face recognition,\n",
    "OCR, and text embedding to power the search capabilities.\n",
    "\n",
    "The code has been refactored to use functional programming principles:\n",
    "- Pure functions where possible\n",
    "- Immutability of data\n",
    "- Higher-order functions\n",
    "- Function composition\n",
    "- Parallel processing for I/O operations\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import face_recognition\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import docx\n",
    "import concurrent.futures\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import dotenv\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple\n",
    "import re\n",
    "import pytesseract\n",
    "import time\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7726d662-0822-4358-aa37-f809ea632667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cache paths\n",
    "CACHE_DIR = 'C:\\\\Users\\\\Rishu\\\\Downloads'\n",
    "\n",
    "# Cache file names\n",
    "CACHE_FILES = {\n",
    "    'text': 'text_features_cache.pkl',\n",
    "    'image': 'image_features_cache.pkl',\n",
    "    'face': 'face_features_cache.pkl',\n",
    "    'image_text': 'image_text_cache.pkl'\n",
    "}\n",
    "\n",
    "# Exclusion lists\n",
    "DEFAULT_EXCLUDE_KEYWORDS = [\n",
    "    \"venv\", \"env\", \"node_modules\", \"__pycache__\",\n",
    "    \"dist-info\", \"macosx\", \"_vendor\", \"thirdpartynotices\"\n",
    "]\n",
    "\n",
    "DEFAULT_EXCLUDE_FILENAMES = [\n",
    "    \"lgpl.txt\", \"vendor.txt\", \"thirdpartysoftwarenotice.txt\", \"entry_points.txt\"\n",
    "]\n",
    "\n",
    "IMAGE_EXTENSIONS = [\".png\", \".jpg\", \".jpeg\"]\n",
    "YOLO_CONFIG = {\n",
    "    'weights': os.environ.get('YOLO_WEIGHTS', 'yolov4.weights'),\n",
    "    'config': os.environ.get('YOLO_CONFIG', 'yolov4.cfg'),\n",
    "    'classes': os.environ.get('YOLO_CLASSES', 'coco.names')\n",
    "}\n",
    "FACE_MATCH_TOLERANCE = float(os.environ.get('FACE_MATCH_TOLERANCE', '0.7'))\n",
    "OBJECT_CONFIDENCE_THRESHOLD = float(os.environ.get('OBJECT_CONFIDENCE_THRESHOLD', '0.8'))\n",
    "\n",
    "# Supported file extensions\n",
    "FILE_EXTENSIONS = [\".pdf\", \".txt\", \".docx\"]\n",
    "\n",
    "# OCR Configuration\n",
    "TESSERACT_CONFIG = os.environ.get('TESSERACT_CONFIG', '')\n",
    "OCR_DPI = int(os.environ.get('OCR_DPI', '300'))\n",
    "\n",
    "# Models configuration\n",
    "DEFAULT_MODEL = \"all-MiniLM-L6-v2\"\n",
    "MODEL_ALIASES = {\n",
    "    \"bge\": \"BAAI/bge-m3\",\n",
    "    \"jina\": \"jinaai/jina-embeddings-v2-base-en\",\n",
    "    \"default\": DEFAULT_MODEL\n",
    "}\n",
    "\n",
    "# Processing configuration\n",
    "DEFAULT_TIMEOUT = 600  # seconds\n",
    "DEFAULT_BATCH_SIZE = 100\n",
    "\n",
    "# Initialize configuration\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9636759-36db-43e2-b264-5152eb661e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Face Recognition Functions ================\n",
    "\n",
    "def extract_face_features(image_path):\n",
    "    \"\"\"\n",
    "    Extract face features from an image using face_recognition library.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = face_recognition.load_image_file(image_path)\n",
    "        face_encodings = face_recognition.face_encodings(image)\n",
    "        return face_encodings\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting face features for {image_path}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2eb1c94-6419-4f4b-a4a5-45e6f4bbcdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Image Recognition Functions ================\n",
    "\n",
    "def load_yolo_model(model_weights='yolov4.weights', config_file='yolov4.cfg', classes_file='coco.names'):\n",
    "    \"\"\"\n",
    "    Load YOLO model and class names.\n",
    "    \n",
    "    Args:\n",
    "        model_weights (str): Path to YOLOv4 weights file\n",
    "        config_file (str): Path to YOLOv4 configuration file\n",
    "        classes_file (str): Path to file containing class names\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (net, output_layers, classes)\n",
    "    \"\"\"\n",
    "    net = cv2.dnn.readNet(model_weights, config_file)\n",
    "    layer_names = net.getLayerNames()\n",
    "    unconnected_out_layers = net.getUnconnectedOutLayers().flatten()\n",
    "    output_layers = [layer_names[i - 1] for i in unconnected_out_layers]\n",
    "    \n",
    "    with open(classes_file, \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    return net, output_layers, classes\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image for YOLO detection.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (blob, img, width, height)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None, None, 0, 0\n",
    "        \n",
    "    height, width, _ = img.shape\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    return blob, img, width, height\n",
    "\n",
    "def extract_objects(net, output_layers, blob, confidence_threshold, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Extract objects from an image using YOLO model.\n",
    "    \n",
    "    Args:\n",
    "        net: YOLO neural network\n",
    "        output_layers: Output layers of the network\n",
    "        blob: Preprocessed image blob\n",
    "        confidence_threshold: Minimum confidence for object detection\n",
    "        image_width: Width of the original image\n",
    "        image_height: Height of the original image\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (bounding_boxes, class_ids, confidences)\n",
    "    \"\"\"\n",
    "    net.setInput(blob)\n",
    "    outputs = net.forward(output_layers)\n",
    "    \n",
    "    bounding_boxes = []\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    \n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > confidence_threshold:\n",
    "                x_center = int(detection[0] * image_width)\n",
    "                y_center = int(detection[1] * image_height)\n",
    "                width = int(detection[2] * image_width)\n",
    "                height = int(detection[3] * image_height)\n",
    "                \n",
    "                x = int(x_center - width / 2)\n",
    "                y = int(y_center - height / 2)\n",
    "                \n",
    "                bounding_boxes.append([x, y, width, height])\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(confidence)\n",
    "                \n",
    "    return bounding_boxes, class_ids, confidences\n",
    "\n",
    "def detect_objects(img_path, net, output_layers, confidence_threshold):\n",
    "    \"\"\"\n",
    "    Detect objects in an image using YOLO.\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): Path to the image file\n",
    "        net: YOLO neural network\n",
    "        output_layers: Output layers of the network\n",
    "        confidence_threshold: Minimum confidence for object detection\n",
    "        \n",
    "    Returns:\n",
    "        list or None: List of detected object features or None if no objects detected\n",
    "    \"\"\"\n",
    "    blob, img, width, height = load_and_preprocess_image(img_path)\n",
    "    \n",
    "    if img is None:\n",
    "        return None\n",
    "        \n",
    "    bounding_boxes, class_ids, confidences = extract_objects(\n",
    "        net, output_layers, blob, confidence_threshold, width, height)\n",
    "    \n",
    "    if bounding_boxes and class_ids and confidences:\n",
    "        features = []\n",
    "        for bbox, cls_id, conf in zip(bounding_boxes, class_ids, confidences):\n",
    "            features.append({\n",
    "                'bounding_box': bbox,\n",
    "                'class_id': cls_id,\n",
    "                'confidence': conf\n",
    "            })\n",
    "        return features\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085c7e5-024b-4a1c-b17d-01c9b05e84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Image Search Functions ================\n",
    "\n",
    "def load_or_extract_objects(folder_path, net, output_layers, confidence_threshold):\n",
    "    \"\"\"\n",
    "    Load cached object features if they exist, otherwise extract and cache them.\n",
    "    \n",
    "    \"\"\"\n",
    "    image_features = load_cache(IMAGE_CACHE)\n",
    "    cached_images = set(image_features.keys())\n",
    "    \n",
    "    # Find all images in the folder\n",
    "    all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"Found {len(all_images)} images in the folder.\")\n",
    "    \n",
    "    # Extract features for new images\n",
    "    new_images = [img for img in all_images if img not in cached_images]\n",
    "    \n",
    "    # Use ThreadPoolExecutor for I/O bound operations\n",
    "    if new_images:\n",
    "        updated_features = {}\n",
    "        print(f\"Processing {len(new_images)} new images...\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Create a partial function with fixed parameters\n",
    "            detect_func = partial(detect_objects, net=net, output_layers=output_layers, \n",
    "                                 confidence_threshold=confidence_threshold)\n",
    "            \n",
    "            # Process images in parallel and track progress\n",
    "            futures = {executor.submit(detect_func, os.path.join(folder_path, img)): img for img in new_images}\n",
    "            \n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(new_images), desc=\"Processing images\"):\n",
    "                img = futures[future]\n",
    "                try:\n",
    "                    features = future.result()\n",
    "                    if features:\n",
    "                        updated_features[img] = features\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img}: {e}\")\n",
    "        \n",
    "        # Update and save cache\n",
    "        image_features.update(updated_features)\n",
    "        save_cache(image_features, IMAGE_CACHE)\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "def search_images_by_query(query, folder_image_features, classes):\n",
    "    \"\"\"\n",
    "    Search for images containing a specific object.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert query to lowercase\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Try to find the class ID for the query\n",
    "    try:\n",
    "        target_id = classes.index(query)\n",
    "    except ValueError:\n",
    "        print(f\"Query '{query}' not found in available classes. Try another term.\\n\")\n",
    "        return []\n",
    "    \n",
    "    # Find images that contain the queried object class\n",
    "    matching_images = []\n",
    "    for filename, features in folder_image_features.items():\n",
    "        if features:  # Check if features exist\n",
    "            for feature in features:\n",
    "                if feature['class_id'] == target_id:\n",
    "                    matching_images.append(filename)\n",
    "                    break  # No need to check other features of this image\n",
    "    \n",
    "    return matching_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343706f-1ad8-4639-8c99-fafc99bff822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Face Search Functions ================\n",
    "\n",
    "def load_or_extract_face_features(folder_path):\n",
    "    \"\"\"\n",
    "    Load cached face features if they exist, otherwise extract and cache them.\n",
    "    \n",
    "    \"\"\"\n",
    "    face_features = load_cache(FACE_CACHE)\n",
    "    cached_images = set(face_features.keys())\n",
    "    \n",
    "    # Find all images in the folder\n",
    "    all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"Found {len(all_images)} images in the folder.\")\n",
    "    \n",
    "    # Extract features for new images\n",
    "    new_images = [img for img in all_images if img not in cached_images]\n",
    "    \n",
    "    # Use ThreadPoolExecutor for I/O bound operations\n",
    "    if new_images:\n",
    "        updated_features = {}\n",
    "        print(f\"Processing {len(new_images)} new images for faces...\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Process images in parallel and track progress\n",
    "            futures = {executor.submit(extract_face_features, os.path.join(folder_path, img)): img for img in new_images}\n",
    "            \n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(new_images), desc=\"Processing faces\"):\n",
    "                img = futures[future]\n",
    "                try:\n",
    "                    encodings = future.result()\n",
    "                    updated_features[img] = encodings\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing face in {img}: {e}\")\n",
    "        \n",
    "        # Update and save cache\n",
    "        face_features.update(updated_features)\n",
    "        save_cache(face_features, FACE_CACHE)\n",
    "    \n",
    "    return face_features\n",
    "\n",
    "def search_images_by_face(input_image_path, face_features_cache):\n",
    "    \"\"\"\n",
    "    Search for images containing faces from the input image.\n",
    "    \n",
    "    Args:\n",
    "        input_image_path (str): Path to the query image containing faces\n",
    "        face_features_cache (dict): Dictionary mapping image filenames to their face encodings\n",
    "        \n",
    "    Returns:\n",
    "        list: List of image filenames containing matching faces\n",
    "    \"\"\"\n",
    "    input_encodings = extract_face_features(input_image_path)\n",
    "    \n",
    "    # Check if any encodings were found in the input image\n",
    "    if not input_encodings:\n",
    "        print(\"No faces found in the input image.\")\n",
    "        return []\n",
    "\n",
    "    matching_images = []\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel face comparison\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        def check_image_for_face_match(item):\n",
    "            img, encodings = item\n",
    "            \n",
    "            # Skip images with no face encodings\n",
    "            if not encodings:\n",
    "                return None\n",
    "                \n",
    "            # Compare each encoding in the cached encodings with the input encodings\n",
    "            for encoding in encodings:\n",
    "                matches = face_recognition.compare_faces(input_encodings, encoding, tolerance=0.7)\n",
    "                if any(matches):  # If there's a match with any of the faces\n",
    "                    return img\n",
    "            return None\n",
    "        \n",
    "        # Process in parallel and track progress\n",
    "        futures = {executor.submit(check_image_for_face_match, item): item[0] \n",
    "                  for item in face_features_cache.items()}\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), \n",
    "                          total=len(face_features_cache), desc=\"Searching faces\"):\n",
    "            try:\n",
    "                match = future.result()\n",
    "                if match:\n",
    "                    matching_images.append(match)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in face comparison: {e}\")\n",
    "    \n",
    "    return matching_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d432690-5279-4a0b-9254-9984b3a6cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Image Text Search Functions ================\n",
    "\n",
    "def extract_text_from_image(img_path):\n",
    "    \"\"\"\n",
    "    Extract text from an image using OCR.\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(img_path)\n",
    "        text = pytesseract.image_to_string(image).strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image {img_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_or_extract_image_text(folder_path, embedding_model):\n",
    "    \"\"\"\n",
    "    Load cached OCR results if they exist, otherwise extract and cache them.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images\n",
    "        embedding_model: Text embedding model\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping image filenames to text content and embeddings\n",
    "    \"\"\"\n",
    "    image_text_cache = load_cache(IMAGE_TEXT_CACHE)\n",
    "    cached_images = set(image_text_cache.keys())\n",
    "    \n",
    "    # Find all images in the folder\n",
    "    all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"Found {len(all_images)} images in the folder.\")\n",
    "    \n",
    "    # Extract text for new images\n",
    "    new_images = [img for img in all_images if img not in cached_images]\n",
    "    \n",
    "    # Use ThreadPoolExecutor for I/O bound operations\n",
    "    if new_images:\n",
    "        updated_cache = {}\n",
    "        print(f\"Processing {len(new_images)} new images for text extraction...\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Process images in parallel for OCR\n",
    "            futures = {executor.submit(extract_text_from_image, \n",
    "                                      os.path.join(folder_path, img)): img for img in new_images}\n",
    "            \n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), \n",
    "                              total=len(new_images), desc=\"Extracting text from images\"):\n",
    "                img = futures[future]\n",
    "                try:\n",
    "                    text = future.result()\n",
    "                    \n",
    "                    # If text found, create embedding\n",
    "                    if text:\n",
    "                        embedding = encode_text(text, embedding_model)\n",
    "                        updated_cache[img] = {\n",
    "                            'text': text,\n",
    "                            'embedding': embedding\n",
    "                        }\n",
    "                    else:\n",
    "                        updated_cache[img] = {\n",
    "                            'text': '',\n",
    "                            'embedding': None\n",
    "                        }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in text extraction for {img}: {e}\")\n",
    "                    updated_cache[img] = {\n",
    "                        'text': '',\n",
    "                        'embedding': None\n",
    "                    }\n",
    "        \n",
    "        # Update and save cache\n",
    "        image_text_cache.update(updated_cache)\n",
    "        save_cache(image_text_cache, IMAGE_TEXT_CACHE)\n",
    "    \n",
    "    return image_text_cache\n",
    "\n",
    "def search_images_by_text(query, image_text_cache, embedding_model):\n",
    "    \"\"\"\n",
    "    Search for images containing text similar to the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Text to search for in images\n",
    "        image_text_cache (dict): Dictionary of image text data\n",
    "        embedding_model: Text embedding model\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tuples (image_filename, similarity_score) sorted by relevance\n",
    "    \"\"\"\n",
    "    query_embedding = encode_text(query, embedding_model)\n",
    "    \n",
    "    results = []\n",
    "    for img, data in image_text_cache.items():\n",
    "        if data['embedding'] is not None:\n",
    "            similarity = cosine_similarity([query_embedding], [data['embedding']])[0][0]\n",
    "            if similarity > 0.3:  # Threshold for text similarity\n",
    "                results.append((img, similarity))\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbf0d8b9-a93c-44bb-a8eb-8dc1d38d1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_image_with_boxes(filename, features, folder_path, classes):\n",
    "    \"\"\"\n",
    "    Plot an image with bounding boxes around detected objects.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Name of the image file\n",
    "        features (list): List of object features with bounding boxes\n",
    "        folder_path (str): Path to the folder containing images\n",
    "        classes (list): List of object class names\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(folder_path, filename)\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    for feature in features:\n",
    "        bbox = feature['bounding_box']\n",
    "        class_id = feature['class_id']\n",
    "        confidence = feature['confidence']\n",
    "        x, y, w, h = bbox\n",
    "        \n",
    "        # Draw rectangle and label\n",
    "        plt.gca().add_patch(plt.Rectangle((x, y), w, h, edgecolor='red', linewidth=2, fill=False))\n",
    "        plt.text(x, y-10, f\"{classes[class_id]}: {confidence:.2f}\", \n",
    "                 bbox=dict(facecolor='red', alpha=0.5), color='white')\n",
    "    \n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "\n",
    "def plot_matching_images(matching_images, intent, folder_path, input_image_path=None):\n",
    "    \"\"\"\n",
    "    Plot matching images found during search.\n",
    "    \n",
    "    Args:\n",
    "        matching_images (list): List of matching image filenames or (filename, score) tuples\n",
    "        intent (str): Type of search ('face', 'image', or 'text_image')\n",
    "        folder_path (str): Path to the folder containing images\n",
    "        input_image_path (str, optional): Path to the input image (for face search)\n",
    "    \"\"\"\n",
    "    if not matching_images:\n",
    "        print(\"No matching images to display.\")\n",
    "        return\n",
    "        \n",
    "    num_images = len(matching_images)\n",
    "    if intent == 'text_image':\n",
    "        # Extract just the filenames for text_image intent\n",
    "        matching_images = [img for img, _ in matching_images]\n",
    "        num_images = len(matching_images)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5 * (1 + (num_images // 3))))\n",
    "    \n",
    "    # For face intent, display the input image as well\n",
    "    plot_offset = 0\n",
    "    if intent == 'face' and input_image_path:\n",
    "        plt.subplot(1 + (num_images // 3), 3, 1)\n",
    "        input_image = Image.open(input_image_path)\n",
    "        plt.imshow(np.array(input_image))\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Input Image\")\n",
    "        plot_offset = 1\n",
    "    \n",
    "    # Plot matching images\n",
    "    for i, result in enumerate(matching_images):\n",
    "        plt.subplot(1 + (num_images // 3), 3, i + 1 + plot_offset)\n",
    "        \n",
    "        if intent == 'text_image':\n",
    "            img_path = os.path.join(folder_path, result)\n",
    "        else:\n",
    "            img_path = os.path.join(folder_path, result)\n",
    "            \n",
    "        img = Image.open(img_path)\n",
    "        plt.imshow(np.array(img))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if intent == 'text_image':\n",
    "            plt.title(f\"Match {i + 1}\")\n",
    "        else:\n",
    "            plt.title(f\"Match {i + 1}\")\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2233191b-6c50-409e-a235-394682705895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    \"\"\"\n",
    "    The program offers four search options:\n",
    "    - image: Search for images based on content description\n",
    "    - face: Search for images containing a specific face\n",
    "    - text_image: Search for images containing specific text\n",
    "    - file: Search for text files based on content similarity\n",
    "    \"\"\"\n",
    "    # Define path to cache files\n",
    "    IMAGE_CACHE = os.path.join(CACHE_DIR, CACHE_FILES['image'])\n",
    "    FACE_CACHE = os.path.join(CACHE_DIR, CACHE_FILES['face'])\n",
    "    IMAGE_TEXT_CACHE = os.path.join(CACHE_DIR, CACHE_FILES['image_text'])\n",
    "    TEXT_CACHE = os.path.join(CACHE_DIR, CACHE_FILES['text'])\n",
    "    \n",
    "    print(\"🔍 What would you like to search for?\")\n",
    "    print(\"Please choose one of the following options:\")\n",
    "    print(\"  🧑‍🦰 face        - Search for images by a face\")\n",
    "    print(\"  🖼️ image       - Search for images using a sample object\")\n",
    "    print(\"  📝 text_image  - Search for images using keywords or text\")\n",
    "    print(\"  📄 file        - Search across all your text files\")\n",
    "    \n",
    "    intent = input(\"\\nEnter your choice (face/image/text_image/file): \").strip().lower()\n",
    "\n",
    "    if intent == \"image\":\n",
    "        \"\"\"\n",
    "        Image search functionality based on visual content recognition.\n",
    "        \n",
    "        Uses an image recognition model to find images matching a query term.\n",
    "        \"\"\"\n",
    "        folder_path = input(\"Enter Folder to search in: \")\n",
    "        print(f\"Using folder path: {folder_path}\")\n",
    "        \n",
    "        # Load the YOLO model\n",
    "        print(\"Loading YOLO model...\")\n",
    "        net, output_layers, classes = load_yolo_model(\n",
    "            model_weights=YOLO_CONFIG['weights'],\n",
    "            config_file=YOLO_CONFIG['config'],\n",
    "            classes_file=YOLO_CONFIG['classes']\n",
    "        )\n",
    "        \n",
    "        # Load or extract object features\n",
    "        print(\"Loading or extracting image features...\")\n",
    "        folder_image_features = load_or_extract_objects(\n",
    "            folder_path, \n",
    "            net, \n",
    "            output_layers, \n",
    "            OBJECT_CONFIDENCE_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # Perform search\n",
    "        query = input(\"Enter your search query (one word): \")\n",
    "        matching_images = search_images_by_query(query, folder_image_features, classes)\n",
    "        \n",
    "        print(f\"Found {len(matching_images)} images matching '{query}'\")\n",
    "        if matching_images:\n",
    "            print(\"Matching Images:\", matching_images)\n",
    "            plot_matching_images(matching_images, intent, folder_path)\n",
    "        else:\n",
    "            print(\"No search results found.\")\n",
    "            \n",
    "    elif intent == \"face\":\n",
    "        \"\"\"\n",
    "        Face search functionality to find images containing a specific face.\n",
    "        \n",
    "        Takes a reference image and finds other images containing the same face.\n",
    "        \"\"\"\n",
    "        folder_path = input(\"Enter Folder to search in: \")\n",
    "        print(f\"Using folder path: {folder_path}\")\n",
    "        \n",
    "        # Load or extract face features\n",
    "        print(\"Loading or extracting face features...\")\n",
    "        face_features_cache = load_or_extract_face_features(folder_path)\n",
    "        \n",
    "        # Perform search\n",
    "        input_image_path = input(\"Enter the path to the input image: \")\n",
    "        matching_images = search_images_by_face(input_image_path, face_features_cache)\n",
    "        \n",
    "        print(f\"Found {len(matching_images)} images with matching faces\")\n",
    "        if matching_images:\n",
    "            print(\"Matching Images:\", matching_images)\n",
    "            plot_matching_images(matching_images, intent, folder_path, input_image_path)\n",
    "        else:\n",
    "            print(\"No matching images found.\")\n",
    "            \n",
    "    elif intent == \"text_image\":\n",
    "        \"\"\"\n",
    "        Text-in-image search functionality to find images containing specific text.\n",
    "        \n",
    "        Uses OCR to identify images that contain the requested text.\n",
    "        \"\"\"\n",
    "        folder_path = input(\"Enter Folder to search in: \")\n",
    "        print(f\"Using folder path: {folder_path}\")\n",
    "        \n",
    "        # Load embedding model\n",
    "        print(\"Loading text embedding model...\")\n",
    "        embedding_model = load_text_embedding_model(DEFAULT_MODEL)\n",
    "        \n",
    "        # Load or extract text from images\n",
    "        print(\"Loading or extracting text from images...\")\n",
    "        image_text_cache = load_or_extract_image_text(folder_path, embedding_model)\n",
    "        \n",
    "        # Perform search\n",
    "        query = input(\"Enter text to search for in images: \")\n",
    "        results = search_images_by_text(query, image_text_cache, embedding_model)\n",
    "        \n",
    "        print(f\"Found {len(results)} images with text similar to '{query}'\")\n",
    "        if results:\n",
    "            print(\"Search Results:\")\n",
    "            for img, similarity in results:\n",
    "                print(f\"Image: {img}, Similarity: {similarity:.4f}\")\n",
    "            plot_matching_images(results, \"text_image\", folder_path)\n",
    "        else:\n",
    "            print(\"No matching images found.\")\n",
    "            \n",
    "    elif intent == 'file':\n",
    "        \"\"\"\n",
    "        File content search functionality to find text files based on content similarity.\n",
    "        \n",
    "        Uses text embeddings to identify files with content similar to the query.\n",
    "        Supports multiple embedding models for comparison.\n",
    "        \"\"\"\n",
    "        folder_path = 'C:\\\\Users\\\\Rishu\\\\Downloads'\n",
    "        print(f\"Using folder path: {folder_path}\")\n",
    "        \n",
    "        # Check if folder exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Error: Folder '{folder_path}' does not exist.\")\n",
    "            exit(1)\n",
    "          \n",
    "        # Select embedding model\n",
    "        model_choice = input(\"Choose model (bge/jina/default): \").strip().lower()\n",
    "        model_name = MODEL_ALIASES.get(model_choice, DEFAULT_MODEL)\n",
    "        print(f\"Using model: {model_name}\")\n",
    "        \n",
    "        # Load embedding model\n",
    "        print(\"Loading text embedding model...\")\n",
    "        embedding_model = load_text_embedding_model(model_name)\n",
    "        \n",
    "        # Load text embeddings cache\n",
    "        text_cache = load_cache(TEXT_CACHE)\n",
    "        \n",
    "        # Get list of files to process\n",
    "        print(f\"Scanning for text files in {folder_path}...\")\n",
    "        file_paths = get_files_in_directory(\n",
    "            folder_path, \n",
    "            FILE_EXTENSIONS,\n",
    "            exclude_keywords=DEFAULT_EXCLUDE_KEYWORDS, \n",
    "            exclude_filenames=DEFAULT_EXCLUDE_FILENAMES\n",
    "        )[:2]\n",
    "        print(f\"Found {len(file_paths)} text files to process\")\n",
    "        \n",
    "        # Show some of the files to verify\n",
    "        if file_paths:\n",
    "            print(\"Sample files:\")\n",
    "            for i, path in enumerate(file_paths[:5]):\n",
    "                print(f\"  {i+1}. {path}\")\n",
    "            if len(file_paths) > 5:\n",
    "                print(f\"  ... and {len(file_paths) - 5} more\")\n",
    "        \n",
    "        # Choose number of workers\n",
    "        max_workers_input = input(\"Maximum number of workers (default: 4): \")\n",
    "        if max_workers_input.strip() and max_workers_input.isdigit():\n",
    "            max_workers = int(max_workers_input)\n",
    "        else:\n",
    "            max_workers = 4\n",
    "        \n",
    "        if file_paths:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            updated_cache, processed_count = index_files(\n",
    "                file_paths=file_paths, \n",
    "                embedding_model=embedding_model, \n",
    "                embeddings_cache=text_cache,\n",
    "                cache_path=TEXT_CACHE,\n",
    "                max_workers=max_workers, \n",
    "                batch_size=DEFAULT_BATCH_SIZE,\n",
    "                timeout=DEFAULT_TIMEOUT,\n",
    "            )\n",
    "            \n",
    "            # Cache is already saved within index_files function\n",
    "            # Just update the local reference\n",
    "            if processed_count > 0:\n",
    "                text_cache = updated_cache\n",
    "                \n",
    "            end_time = time.time()\n",
    "            print(f\"Indexing completed in {end_time - start_time:.2f} seconds\")\n",
    "        else:\n",
    "            print(\"No files to index.\")\n",
    "        \n",
    "        # Search the files\n",
    "        query = input(\"Enter your text query: \")\n",
    "        print(f\"Searching for: '{query}'\")\n",
    "        \n",
    "        results = search_files(query, text_cache, embedding_model)\n",
    "        \n",
    "        print(\"\\nSearch Results:\")\n",
    "        if results:\n",
    "            for i, (file_path, similarity) in enumerate(results):\n",
    "                print(f\"{i+1}. File: {file_path}\")\n",
    "                print(f\"   Similarity: {similarity:.4f}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"No matching files found.\")\n",
    "    else:\n",
    "        print(\"Invalid option. Please choose from: face, image, text_image, or file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76fe6c55-a4d7-4d25-9bad-5f3a279c7944",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenProcessPool\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m numbers = [\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m concurrent.futures.ProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msquare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumbers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)  \u001b[38;5;66;03m# Output: [1, 4, 9, 16, 25]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\process.py:647\u001b[39m, in \u001b[36m_chain_from_iterable_of_lists\u001b[39m\u001b[34m(iterable)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[32m    642\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[33;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[33;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[32m    645\u001b[39m \u001b[33;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[32m    646\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mBrokenProcessPool\u001b[39m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def square(n):\n",
    "    return n * n\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        results = list(executor.map(square, numbers))\n",
    "\n",
    "    print(results)  # Output: [1, 4, 9, 16, 25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa39646-76b4-4709-916f-e67148dfc272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
